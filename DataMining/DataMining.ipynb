{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(dataset_path):\n",
        "    \"\"\"Load dataset from the provided path.\"\"\"\n",
        "    return pd.read_csv(dataset_path)\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"Clean the dataset by handling missing values.\"\"\"\n",
        "    df = df.dropna()  # Drop rows with missing values\n",
        "    # Alternatively, you could fill missing values with df.fillna()\n",
        "    return df\n",
        "\n",
        "def encode_categorical(df, columns):\n",
        "    \"\"\"Encode categorical columns to numeric.\"\"\"\n",
        "    df = pd.get_dummies(df, columns=columns, drop_first=True)\n",
        "    return df\n",
        "\n",
        "def scale_features(df, features):\n",
        "    \"\"\"Standardize the numeric features.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    df[features] = scaler.fit_transform(df[features])\n",
        "    return df\n",
        "\n",
        "def split_data(df, target, test_size=0.2):\n",
        "    \"\"\"Split data into training and testing sets.\"\"\"\n",
        "    X = df.drop(target, axis=1)\n",
        "    y = df[target]\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=42)"
      ],
      "metadata": {
        "id": "G4lMxYG32aQr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_correlation_matrix(df):\n",
        "    \"\"\"Plot the correlation matrix for numerical features.\"\"\"\n",
        "    corr_matrix = df.corr()\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt='.2f')\n",
        "    plt.title(\"Correlation Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_distribution(df, feature):\n",
        "    \"\"\"Plot distribution of a specific feature.\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(df[feature], kde=True)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.show()\n",
        "\n",
        "def plot_pairplot(df):\n",
        "    \"\"\"Plot pairplot of numerical features to examine relationships.\"\"\"\n",
        "    sns.pairplot(df)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PcI7OUKP1QKo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_logistic_regression(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate Logistic Regression.\"\"\"\n",
        "    model = LogisticRegression(max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def train_decision_tree(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate Decision Tree Classifier.\"\"\"\n",
        "    model = DecisionTreeClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def train_random_forest(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate Random Forest Classifier.\"\"\"\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "VmHAJUiy1gox"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def kmeans_clustering(df, num_clusters=3):\n",
        "    \"\"\"Perform KMeans clustering on the dataset.\"\"\"\n",
        "    model = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    df['Cluster'] = model.fit_predict(df)\n",
        "    return df\n",
        "\n",
        "def plot_clusters(df, x_col, y_col):\n",
        "    \"\"\"Plot the clusters in 2D space.\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=df[x_col], y=df[y_col], hue=df['Cluster'], palette='Set2')\n",
        "    plt.title(f'KMeans Clustering ({x_col} vs {y_col})')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8vjyZwvU1jv_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "def load_transactions_data(file_path):\n",
        "    \"\"\"Load supermarket transactions data.\"\"\"\n",
        "    return pd.read_csv(file_path, header=None)\n",
        "\n",
        "def encode_transactions(df):\n",
        "    \"\"\"Convert data into one-hot encoded format.\"\"\"\n",
        "    df_encoded = df.stack().str.get_dummies().sum(level=0)\n",
        "    return df_encoded\n",
        "\n",
        "def mine_association_rules(df_encoded, min_support=0.1, min_threshold=0.5):\n",
        "    \"\"\"Mine association rules using Apriori Algorithm.\"\"\"\n",
        "    frequent_itemsets = apriori(df_encoded, min_support=min_support, use_colnames=True)\n",
        "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_threshold)\n",
        "    return rules"
      ],
      "metadata": {
        "id": "vm0hG7-A4UEW"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}